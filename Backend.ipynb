{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d849eafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import base64\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "import openai\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from flask import Flask, request, render_template, flash, redirect\n",
    "from werkzeug.utils import secure_filename\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.graphs import Neo4jGraph\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a264675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IshiTa\\AppData\\Local\\Temp\\ipykernel_7132\\1556466422.py:12: LangChainDeprecationWarning: The class `Neo4jGraph` was deprecated in LangChain 0.3.8 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-neo4j package and should be used instead. To use it run `pip install -U :class:`~langchain-neo4j` and import as `from :class:`~langchain_neo4j import Neo4jGraph``.\n",
      "  graph = Neo4jGraph(url=NEO4J_URL, username=NEO4J_USERNAME, password=NEO4J_PASSWORD)\n",
      "C:\\Users\\IshiTa\\AppData\\Local\\Temp\\ipykernel_7132\\1556466422.py:13: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(openai_api_key=openai.api_key, temperature=0, model_name=\"gpt-4.1\")\n"
     ]
    }
   ],
   "source": [
    "# Flask config\n",
    "app = Flask(__name__)\n",
    "app.secret_key = \"super_secret\"  \n",
    "\n",
    "# API and DB credentials \n",
    "openai.api_key = \"\" \n",
    "NEO4J_URL = \"bolt://localhost:7687\"\n",
    "NEO4J_USERNAME = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"hawa4545\"\n",
    "\n",
    "# Setup models and services\n",
    "graph = Neo4jGraph(url=NEO4J_URL, username=NEO4J_USERNAME, password=NEO4J_PASSWORD)\n",
    "llm = ChatOpenAI(openai_api_key=openai.api_key, temperature=0, model_name=\"gpt-4.1\")\n",
    "neo4j_driver = GraphDatabase.driver(NEO4J_URL, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\",use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794a640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'tiff'}\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4e2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_embedding_from_base64(base64_image):\n",
    "    try:\n",
    "        image_bytes = base64.b64decode(base64_image)\n",
    "        image = Image.open(BytesIO(image_bytes)).convert(\"RGB\")\n",
    "        inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.get_image_features(**inputs)\n",
    "        return image_features.cpu().numpy().flatten()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing base64 image: {e}\")\n",
    "        return np.random.rand(512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ada121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_node_embeddings(tx):\n",
    "    query = \"\"\"\n",
    "    MATCH (ot:OntologyTerm)\n",
    "    WHERE ot.node2Vec IS NOT NULL AND ot.id IS NOT NULL\n",
    "    RETURN ot.id AS id, ot.node2Vec AS embedding\n",
    "    \"\"\"\n",
    "    result = tx.run(query)\n",
    "    embeddings = {}\n",
    "    for record in result:\n",
    "        embeddings[record[\"id\"]] = np.array(record[\"embedding\"])\n",
    "    return embeddings\n",
    "\n",
    "def find_similar_nodes(target_embedding, all_embeddings, top_n=20):\n",
    "    similarities = {}\n",
    "    for node_id, embedding in all_embeddings.items():\n",
    "        similarity = cosine_similarity(\n",
    "            target_embedding.reshape(1, -1), embedding.reshape(1, -1)\n",
    "        )[0][0]\n",
    "        similarities[node_id] = similarity\n",
    "    return sorted(similarities.items(), key=lambda item: item[1], reverse=True)[:top_n]\n",
    "\n",
    "def get_node_info_by_ids(tx, node_ids):\n",
    "    query = \"\"\"\n",
    "    MATCH (ot:OntologyTerm)\n",
    "    WHERE ot.id IN $node_ids\n",
    "    RETURN ot.id AS id\n",
    "    \"\"\"\n",
    "    result = tx.run(query, node_ids=node_ids)\n",
    "    return [record[\"id\"] for record in result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51adf14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_similar_terms_from_neo4j(image_embedding):\n",
    "    with neo4j_driver.session() as session:\n",
    "        all_embeddings = session.execute_read(get_all_node_embeddings)\n",
    "        similar_nodes = find_similar_nodes(image_embedding, all_embeddings)\n",
    "        similar_node_ids = [node_id for node_id, _ in similar_nodes]\n",
    "        return session.execute_read(get_node_info_by_ids, similar_node_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df0f66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image_path):\n",
    "    try:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    except Exception as e:\n",
    "        print(f\"Encoding error: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b821fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_messages_base64 = [\n",
    "     {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Mild Dementia/OAS1_0028_MR1_mpr-1_100.jpg')}\"}}]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Mild Dementia\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Moderate Dementia/OAS1_0308_MR1_mpr-1_100.jpg')}\"}}]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Moderate Dementia\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Very Mild Dementia/OAS1_0003_MR1_mpr-1_100.jpg')}\"}}]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Very Mild Dementia\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Non Demented/OAS1_0112_MR1_mpr-2_149.jpg')}\"}}]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Non Demented\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Very Mild Dementia/OAS1_0003_MR1_mpr-1_113.jpg')}\"}}]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Very Mild Dementia\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Mild Dementia/OAS1_0028_MR1_mpr-1_112.jpg')}\"}}]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Mild Dementia\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Very Mild Dementia/OAS1_0003_MR1_mpr-1_116.jpg')}\"}}]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Very Mild Dementia\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Mild Dementia/OAS1_0028_MR1_mpr-1_123.jpg')}\"}}]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Mild Dementia\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Non Demented/OAS1_0112_MR1_mpr-3_107.jpg')}\"}}]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Non Demented\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Non Demented/OAS1_0112_MR1_mpr-3_111.jpg')}\"}}]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Non Demented\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Moderate Dementia/OAS1_0308_MR1_mpr-1_118.jpg')}\"}}]},\n",
    "    {\"role\": \"assistant\", \"content\": \"Moderate Dementia\"},\n",
    "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Classify this MRI scan.\"}, {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encode_image_to_base64(r'D:/project/Data/Moderate Dementia/OAS1_0308_MR1_mpr-1_123.jpg')}\"}}]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149afc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_prompt_with_embeddings = \"\"\"\n",
    "You are a medical AI assistant. Analyze the MRI image and consider its features.\n",
    "Compare the image's features with concepts in the knowledge graph, using embeddings to find similar concepts.\n",
    "Use the IDs of these similar concepts from the graph, along with the image and its features and few-shot examples,\n",
    "to make an accurate diagnosis. Reply with the category: Moderate Dementia, Mild Dementia, Very Mild Dementia, Non Demented.\n",
    "Also use other concepts in knowledge graph like Diagnosis stage etc. \n",
    "Here is the schema:\n",
    "Node Types:\n",
    "MRI_Image:\n",
    "Properties: image_path, white_matter_lesions, cortical_thickness, ventricle_size, diagnosis\n",
    "Relationships: [:HAS_DIAGNOSIS] -> DiagnosisStage\n",
    "DiagnosisStage:\n",
    "Properties: name, avg_white_matter_lesions, avg_cortical_thickness, avg_ventricle_size\n",
    "OntologyTerm:\n",
    "Properties: id, name, description, node2vec (embedding vector)\n",
    "Relationships: [:RELATIONSHIP {type: \"is_a\" | \"part_of\"}] -> OntologyTerm\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d7f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_professional_report(report_text):\n",
    "    sections = {\n",
    "        \"technique\": \"\",\n",
    "        \"findings\": \"\",\n",
    "        \"impression\": \"\",\n",
    "        \"diagnosis\": \"\",\n",
    "        \"recommendations\": \"\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Clean up the report text\n",
    "        report_text = report_text.replace(\"**\", \"\").replace(\"\\n\\n\", \"\\n\").strip()\n",
    "        \n",
    "        # Extract Technique\n",
    "        technique_match = re.search(r\"TECHNIQUE:(.+?)(?=FINDINGS:|\\Z)\", report_text, re.DOTALL)\n",
    "        if technique_match:\n",
    "            sections[\"technique\"] = technique_match.group(1).strip()\n",
    "        else:\n",
    "            sections[\"technique\"] = \"Multiplanar MRI sequences were obtained including T1-weighted, T2-weighted, and FLAIR images.\"\n",
    "        \n",
    "        # Extract Findings\n",
    "        findings_match = re.search(r\"FINDINGS:(.+?)(?=IMPRESSION:|\\Z)\", report_text, re.DOTALL)\n",
    "        if findings_match:\n",
    "            findings_content = findings_match.group(1).strip()\n",
    "            # Clean up findings bullets\n",
    "            findings_content = findings_content.replace(\"* \", \"• \")\n",
    "            sections[\"findings\"] = findings_content\n",
    "        \n",
    "        # Extract Impression\n",
    "        impression_match = re.search(r\"IMPRESSION:(.+?)(?=DIAGNOSIS:|RECOMMENDATIONS:|\\Z)\", report_text, re.DOTALL)\n",
    "        if impression_match:\n",
    "            impression_content = impression_match.group(1).strip()\n",
    "            # Number the impression points properly\n",
    "            impression_content = re.sub(r\"(\\d+\\.)\\s+\", r\"\\1 \", impression_content)\n",
    "            sections[\"impression\"] = impression_content\n",
    "        \n",
    "        # Enhanced Diagnosis extraction\n",
    "        diagnosis = \"\"\n",
    "        # First try to extract from DIAGNOSIS section\n",
    "        diagnosis_match = re.search(r\"DIAGNOSIS:(.+?)(?=RECOMMENDATIONS:|\\Z)\", report_text, re.DOTALL)\n",
    "        if diagnosis_match:\n",
    "            diagnosis = diagnosis_match.group(1).strip()\n",
    "        else:\n",
    "            # Then try to extract from IMPRESSION section\n",
    "            impression = sections.get(\"impression\", \"\")\n",
    "            diagnosis_match = re.search(\n",
    "                r\"Primary diagnosis[:\\-]?\\s*(.+?)(?:\\s*(?:based on|due to|with|,)|$)\",\n",
    "                impression,\n",
    "                re.IGNORECASE\n",
    "            )\n",
    "            if diagnosis_match:\n",
    "                diagnosis = diagnosis_match.group(1).strip()\n",
    "                # Clean up diagnosis text\n",
    "                diagnosis = re.sub(r\"^\\W+|\\W+$\", \"\", diagnosis)  # Remove surrounding punctuation\n",
    "                diagnosis = diagnosis.split(\",\")[0]  # Take only first part if comma separated\n",
    "        \n",
    "        # Set the diagnosis, defaulting to \"Diagnosis not specified\" if empty\n",
    "        sections[\"diagnosis\"] = diagnosis if diagnosis else \"Diagnosis not specified\"\n",
    "        \n",
    "        # Extract Recommendations\n",
    "        recommendations_match = re.search(r\"RECOMMENDATIONS:(.+)\", report_text, re.DOTALL)\n",
    "        if recommendations_match:\n",
    "            recommendations_content = recommendations_match.group(1).strip()\n",
    "            # Number the recommendations properly\n",
    "            recommendations_content = re.sub(r\"(\\d+\\.)\\s+\", r\"\\1 \", recommendations_content)\n",
    "            sections[\"recommendations\"] = recommendations_content\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"Parsing error:\", e)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23e89e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        if 'file' not in request.files:\n",
    "            flash(\"No file part\")\n",
    "            return redirect(request.url)\n",
    "\n",
    "        file = request.files['file']\n",
    "        if file.filename == '':\n",
    "            flash(\"No selected file\")\n",
    "            return redirect(request.url)\n",
    "\n",
    "        if file and allowed_file(file.filename):\n",
    "            try:\n",
    "                # Read the uploaded image\n",
    "                image_bytes = file.read()\n",
    "                base64_image = base64.b64encode(image_bytes).decode('utf-8')\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Generate image embedding and fetch related terms from Neo4j\n",
    "                image_embedding = generate_image_embedding_from_base64(base64_image)\n",
    "                similar_terms_info = fetch_similar_terms_from_neo4j(image_embedding)\n",
    "                embedding_context = (\n",
    "                    \"Related Ontology Term IDs: \" + \", \".join(similar_terms_info)\n",
    "                    if similar_terms_info else \"No related concepts found in the knowledge graph.\"\n",
    "                )\n",
    "\n",
    "                # Prompt for GPT-4.1\n",
    "                prompt_text = (\n",
    "                    f\"Generate a professional radiology report for this MRI brain scan using the image features and \"\n",
    "                    f\"graph-based knowledge context provided below.\\n\\n\"\n",
    "                    f\"Knowledge Graph Context (based on image features and node embeddings): {embedding_context}.\\n\\n\"\n",
    "                    \"Follow this exact structure and clinical formatting:\\n\\n\"\n",
    "                    \"MRI BRAIN - DEMENTIA ASSESSMENT REPORT\\n\\n\"\n",
    "                    \"TECHNIQUE:\\n\"\n",
    "                    \"Multiplanar MRI sequences were obtained including T1-weighted, T2-weighted, and FLAIR images.\\n\\n\"\n",
    "                    \"FINDINGS:\\n\"\n",
    "                    \"- Brain parenchyma: Describe volume loss, asymmetry, or preservation\\n\"\n",
    "                    \"- Ventricular system: Note size, symmetry, or any dilation\\n\"\n",
    "                    \"- Sulci and gyri: Comment on cortical thinning or widening\\n\"\n",
    "                    \"- White matter: Describe hyperintensities or demyelination\\n\"\n",
    "                    \"- Basal ganglia and thalamus: Mention any signal abnormalities or structural changes\\n\"\n",
    "                    \"- Posterior fossa: Describe cerebellum and brainstem appearance\\n\\n\"\n",
    "                    \"IMPRESSION:\\n\"\n",
    "                    \"1. Primary diagnosis: [Exactly one of: Non Demented, Very Mild Dementia, Mild Dementia, or Moderate Dementia] explain based on key imaging and graph-derived findings\\n\"\n",
    "                    \"2. Supporting evidence:Specific features observed in the scan and relevant graph-derived contex\\n\"\n",
    "                    \"3. Differential considerations: Only list if graph features suggest ambiguity.Prioritize by biological plausibility. Include atypical presentations if indicated.Don't reference about graph  just use it and give diafferential consideration.\\n\\n\"\n",
    "                    \"DIAGNOSIS:\\n\"\n",
    "                    \"[Repeat the exact diagnosis category from the Impression section here]\\n\\n\"\n",
    "                    \"RECOMMENDATIONS:\\n\"\n",
    "                    \"Clinical correlation with neuropsychological testing\\n\"\n",
    "                    \"Follow-up imaging or assessment timeline if necessary\\n\"\n",
    "                    \"Add more reccomendations based on diagnosis atleast 4 to 6\\n\"\n",
    "                    \"Consider referral to neurology or memory clinic if warranted\\n\\n\"\n",
    "                    \"Important Notes:\\n\"\n",
    "                    \"- The diagnosis must appear in both IMPRESSION and DIAGNOSIS sections\\n\"\n",
    "                    \"- Use only the exact diagnosis categories listed above\\n\"\n",
    "                    \"- Keep language professional and concise\\n\"\n",
    "                    \"- Do not mention the knowledge graph in the report\"\n",
    "                )\n",
    "\n",
    "                # Construct prompt for GPT with image and text\n",
    "                test_image_prompt = {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": prompt_text},\n",
    "                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                    ]\n",
    "                }\n",
    "\n",
    "                # GPT-4.1 API call\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-4.1\",\n",
    "                    messages=[{\"role\": \"system\", \"content\": schema_prompt_with_embeddings}] + few_shot_messages_base64 + [test_image_prompt],\n",
    "                    max_tokens=1000\n",
    "                )\n",
    "\n",
    "                full_response = response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "                # Parse with the improved professional parser\n",
    "                report_data = parse_professional_report(full_response)\n",
    "\n",
    "                # Calculate processing time and get timestamp\n",
    "                processing_time = round(time.time() - start_time, 2)\n",
    "                timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                return render_template(\n",
    "                    \"result.html\",\n",
    "                    report_data=report_data,\n",
    "                    image_data=base64_image,\n",
    "                    timestamp=timestamp,\n",
    "                    processing_time=processing_time\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                flash(f\"Error processing image: {e}\")\n",
    "                return redirect(request.url)\n",
    "\n",
    "    return render_template('index.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "152bf284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5001\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [23/May/2025 09:32:52] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/May/2025 09:32:52] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [23/May/2025 09:34:51] \"POST / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(port=5001, debug=False, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt4o-mri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
